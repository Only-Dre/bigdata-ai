{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OktCQHhqE0ie"
      },
      "outputs": [],
      "source": [
        "# Fazendo as importações\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# 1. Carregar o dataset\n",
        "df = pd.read_csv(\"earthquake_data_tsunami.csv\")\n",
        "\n",
        "print(\"\\nPrimeiras linhas do dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nInformações do dataset:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nEstatísticas descritivas:\")\n",
        "print(df.describe())\n",
        "\n",
        "# 2. Separar X e y\n",
        "X = df.drop(\"tsunami\", axis=1)   # todas as colunas menos a coluna alvo\n",
        "y = df[\"tsunami\"]                # coluna alvo (0 ou 1)\n",
        "\n",
        "# 3. Normalizar os dados (IMPORTANTE)\n",
        "escalador = StandardScaler()\n",
        "X_escalado = escalador.fit_transform(X)\n",
        "\n",
        "# 4. Dividir em treino e teste (70% / 30%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_escalado, y, test_size=0.30, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# 5. Criar modelos de forma simples\n",
        "modelos = {\n",
        "    \"Regressão Logística\": LogisticRegression(max_iter=1000),\n",
        "    \"Árvore de Decisão\": DecisionTreeClassifier(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=200),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "resultados = {}\n",
        "\n",
        "# 6. Treinar, prever e calcular métricas\n",
        "for nome, modelo in modelos.items():\n",
        "    modelo.fit(X_train, y_train)\n",
        "    y_pred = modelo.predict(X_test)\n",
        "\n",
        "    # calcular métricas\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "    resultados[nome] = {\n",
        "        \"Acurácia\": acc,\n",
        "        \"Precisão\": prec,\n",
        "        \"Recall\": rec,\n",
        "        \"F1-Score\": f1\n",
        "    }\n",
        "\n",
        "# 7. Mostrar os resultados\n",
        "print(\"\\n=== RESULTADOS DOS MODELOS ===\")\n",
        "for nome, metrica in resultados.items():\n",
        "    print(f\"\\nModelo: {nome}\")\n",
        "    for m, v in metrica.items():\n",
        "        print(f\"  {m}: {v:.4f}\")\n",
        "\n",
        "# 8. Escolher o melhor modelo (baseado em Recall)\n",
        "melhor = max(resultados.items(), key=lambda x: x[1][\"Recall\"])\n",
        "print(\"\\nMelhor modelo segundo Recall:\", melhor[0])\n"
      ]
    }
  ]
}